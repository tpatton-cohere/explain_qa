{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers ipywidgets jupyter tensorflow textualheatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForQuestionAnswering: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ays \n",
      "the pain mainly goes down just to his calf and not really his foot he says occasionally goes to his foot but is mostly in his \n",
      "thigh and calf. aggravating factor activity alleviating factors are rest. he reports weakness in his right lower extremity as \n",
      "well. he says this occurs when he has flareups but currently does not have the weakness.. denies any numbness or \n",
      "tingling. he says had pain since 1975 in his back. he occasionally has flareups but these have gotten worse and worse. \n",
      "he is done physical therapy and this made him worse. he has been offered injections but is refused these. he is \n",
      "concerned about his diabetes. i have offered him injection several times. he does not wish to have any. \n",
      "physical examination: \n",
      "alert and oriented x 4. patient in no acute distress. normal affect and mentation. \n",
      "the patient stands with the shoulders and pelvis level. sagittal and frontal contours of the spine are well maintained. \n",
      "examination of the low back region reveals the skin to be int\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "from textualheatmap import TextualHeatmap\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert_model/\")\n",
    "model = TFBertForQuestionAnswering.from_pretrained(\"./bert_model/\", from_pt=True)\n",
    "\n",
    "with open('./sample.txt') as f:\n",
    "    context = f.read()[500:1500]\n",
    "        \n",
    "question = \"Does the paitent feel numb?\"\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ays \n",
      "the pain mainly goes down just to his calf and not really his foot he says occasionally goes to his foot but is mostly in his \n",
      "thigh and calf. aggravating factor activity alleviating factors are rest. he reports weakness in his right lower extremity as \n",
      "well. he says this occurs when he has flareups but currently does not have the weakness.. denies any numbness or \n",
      "tingling. he says had pain since 1975 in his back. he occasionally has flareups but these have gotten worse and worse. \n",
      "he is done physical therapy and this made him worse. he has been offered injections but is refused these. he is \n",
      "concerned about his diabetes. i have offered him injection several times. he does not wish to have any. \n",
      "physical examination: \n",
      "alert and oriented x 4. patient in no acute distress. normal affect and mentation. \n",
      "the patient stands with the shoulders and pelvis level. sagittal and frontal contours of the spine are well maintained. \n",
      "examination of the low back region reveals the skin to be int\n"
     ]
    }
   ],
   "source": [
    "with open('./sample.txt') as f:\n",
    "    context = f.read()[500:1500]\n",
    "        \n",
    "question = \"Does the paitent have diabetes?\"\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_scores(scores, best_idx, token_ids_size):\n",
    "    \"\"\"\n",
    "    Unhappy this needs to be its own function\n",
    "    \"\"\"\n",
    "    mask = np.zeros((1, token_ids_size))\n",
    "    mask[0, best_idx] = 1\n",
    "    mask_tensor = tf.constant(mask, dtype='float32')\n",
    "    masked_scores = scores * mask_tensor\n",
    "    return masked_scores\n",
    "    \n",
    "\n",
    "def get_gradient(question, context, model, tokenizer): \n",
    "    embedding_matrix = model.get_input_embeddings()\n",
    "    encoded_tokens =  tokenizer.encode_plus(question, context, add_special_tokens=True, return_token_type_ids=True, return_tensors=\"tf\")\n",
    "    token_ids = list(encoded_tokens[\"input_ids\"].numpy()[0])\n",
    "    vocab_size = embedding_matrix.vocab_size\n",
    "\n",
    "    # convert token ids to one hot. We can't differentiate wrt to int token ids hence the need for one hot representation\n",
    "    token_ids_tensor = tf.constant([token_ids], dtype='int32')\n",
    "    token_ids_tensor_one_hot = tf.one_hot(token_ids_tensor, vocab_size) \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # (i) watch input variable\n",
    "        tape.watch(token_ids_tensor_one_hot)\n",
    "\n",
    "        # multiply input model embedding matrix; allows us do backprop wrt one hot input \n",
    "        inputs_embeds = tf.matmul(token_ids_tensor_one_hot, embedding_matrix.weights[0]) \n",
    "        \n",
    "        # (ii) get prediction\n",
    "        output = model({\"inputs_embeds\": inputs_embeds, \"token_type_ids\": encoded_tokens[\"token_type_ids\"], \"attention_mask\": encoded_tokens[\"attention_mask\"] })\n",
    "        start_scores = output.start_logits\n",
    "        end_scores = output.end_logits\n",
    "        best_start = np.argmax(start_scores)\n",
    "        best_end = np.argmax(end_scores) + 1\n",
    "        \n",
    "        mask_start_scores = mask_scores(start_scores, best_start, len(token_ids))\n",
    "        mask_end_scores = mask_scores(end_scores, best_end, len(token_ids))\n",
    "        \n",
    "        gradients = tf.norm(tape.gradient([mask_start_scores, mask_end_scores], token_ids_tensor_one_hot), axis=2)\n",
    "        \n",
    "        token_words = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        answer_text = tokenizer.decode(token_ids[best_start:best_end])\n",
    "        token_types = list(encoded_tokens[\"token_type_ids\"].numpy()[0])\n",
    "        \n",
    "        print(answer_text)\n",
    "        \n",
    "        return gradients.numpy()[0], token_words, answer_text, token_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n"
     ]
    }
   ],
   "source": [
    "gradients, token_words, answer_text, token_types = get_gradient(question, context, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>gradient</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.095414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does</td>\n",
       "      <td>0.511312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>0.105904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>0.286545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##ait</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>to</td>\n",
       "      <td>0.015552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>be</td>\n",
       "      <td>0.012456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>in</td>\n",
       "      <td>0.025764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>##t</td>\n",
       "      <td>0.021678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.035375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  gradient  type\n",
       "0    [CLS]  0.095414     0\n",
       "1     Does  0.511312     0\n",
       "2      the  0.105904     0\n",
       "3        p  0.286545     0\n",
       "4    ##ait  1.000000     0\n",
       "..     ...       ...   ...\n",
       "219     to  0.015552     1\n",
       "220     be  0.012456     1\n",
       "221     in  0.025764     1\n",
       "222    ##t  0.021678     1\n",
       "223  [SEP]  0.035375     1\n",
       "\n",
       "[224 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_df = pd.DataFrame({'word' : token_words,\n",
    "                        'gradient' : gradients,\n",
    "                        'type' : token_types})\n",
    "\n",
    "grad_df['gradient'] = grad_df['gradient'] / grad_df['gradient'].max()\n",
    "grad_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = grad_df[grad_df['type'] == 0]\n",
    "answer = grad_df[grad_df['type'] == 1]\n",
    "max_grad_idx = np.argmax(answer['gradient'].to_numpy())\n",
    "focused = answer.iloc[max_grad_idx-50:max_grad_idx+50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: hsla(110, 70%, 50%, 0.06736859548836946);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08576461561024189);\">says</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06397669203579426);\">had</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07933117188513279);\">pain</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06335645113140345);\">since</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07062607575207949);\">1975</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05964017612859607);\">in</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05822975551709533);\">his</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07105027865618467);\">back</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08007399700582027);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08299961313605309);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08486946932971477);\">occasionally</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06887855343520641);\">has</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08935021795332432);\">flare</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07993873544037342);\">##ups</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06260614972561598);\">but</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06743079442530871);\">these</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07199771851301193);\">have</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0749655481427908);\">gotten</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07785866186022758);\">worse</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.060462353657931095);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07612450011074542);\">worse</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08506702110171319);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06153953913599253);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06200130339711905);\">is</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07591570131480693);\">done</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07290087826550007);\">physical</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06753550935536623);\">therapy</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0595605512149632);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.058055761735886335);\">this</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.062311573140323165);\">made</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06760019473731518);\">him</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0759347453713417);\">worse</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0843478087335825);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0674636984243989);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.061045899614691736);\">has</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06638367772102356);\">been</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10416000336408615);\">offered</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08411017656326295);\">injection</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06349391620606185);\">##s</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08515842109918595);\">but</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0994413174688816);\">is</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.1744756981730461);\">refused</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.11369720846414566);\">these</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.15919943898916245);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.13845566064119338);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10263400673866271);\">is</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.2812395840883255);\">concerned</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.18297853171825407);\">about</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.15253425389528275);\">his</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.3143547862768173);\">diabetes</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.1436219483613968);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.12369900718331336);\">i</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08532873541116714);\">have</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10551423057913781);\">offered</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09063845202326774);\">him</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07993151471018792);\">injection</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06959739420562983);\">several</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07150071635842323);\">times</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0928870014846325);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0675833985209465);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06383641846477986);\">does</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06746844444423913);\">not</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09835276305675507);\">wish</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05931097837164998);\">to</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06209009308367968);\">have</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06492639277130366);\">any</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.11826090291142463);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09081170000135899);\">physical</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08493025228381157);\">examination</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09217289760708809);\">:</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06817355565726757);\">alert</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06042493246495724);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07579546794295311);\">oriented</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07061886545270682);\">x</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06473593786358833);\">4</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06409325636923313);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07093384340405465);\">patient</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05959723964333535);\">in</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06046277023851872);\">no</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06343498341739177);\">acute</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07096224129199982);\">distress</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07376317791640759);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07705159336328507);\">normal</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06535624135285616);\">affect</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06179390233010054);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0637508923187852);\">men</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08521993942558766);\">##tation</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06988966688513756);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05903477957472206);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08704848699271679);\">patient</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07347380891442298);\">stands</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06836602631956339);\">with</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05970107382163406);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.11081950888037681);\">shoulders</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06869464255869388);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10471079722046853);\">p</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09367073625326157);\">##el</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08602805100381375);\">##vis</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07463661767542362);\">level</span><span> </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def get_html_str(words, gradients):\n",
    "    ret_str = ''\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        grad = int(gradients[i] * 25 + 75) - 15\n",
    "        grad_f = gradients[i] * 0.7 + 0.05\n",
    "        ret_str += f'<span style=\"background-color: hsla(110, 70%, 50%, {grad_f});\">{word}</span><span> </span>'\n",
    "        \n",
    "    return ret_str\n",
    "        \n",
    "heatmap = get_html_str(focused['word'].to_list(), focused['gradient'].to_list())\n",
    "display(\n",
    "    HTML(heatmap)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take gradient wrt correct output? DONE\n",
    "# fix from_pt error? DONE\n",
    "# better output? DONE\n",
    "# clean tokens\n",
    "# package all of this up into a .py?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING GRADIENTS.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradients as grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForQuestionAnswering: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert_model/\")\n",
    "model = TFBertForQuestionAnswering.from_pretrained(\"./bert_model/\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ays \n",
      "the pain mainly goes down just to his calf and not really his foot he says occasionally goes to his foot but is mostly in his \n",
      "thigh and calf. aggravating factor activity alleviating factors are rest. he reports weakness in his right lower extremity as \n",
      "well. he says this occurs when he has flareups but currently does not have the weakness.. denies any numbness or \n",
      "tingling. he says had pain since 1975 in his back. he occasionally has flareups but these have gotten worse and worse. \n",
      "he is done physical therapy and this made him worse. he has been offered injections but is refused these. he is \n",
      "concerned about his diabetes. i have offered him injection several times. he does not wish to have any. \n",
      "physical examination: \n",
      "alert and oriented x 4. patient in no acute distress. normal affect and mentation. \n",
      "the patient stands with the shoulders and pelvis level. sagittal and frontal contours of the spine are well maintained. \n",
      "examination of the low back region reveals the skin to be int\n"
     ]
    }
   ],
   "source": [
    "with open('./sample.txt') as f:\n",
    "    context = f.read()[500:1500]\n",
    "        \n",
    "question = \"Does the patient feel numb?\"\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>gradient</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.078237</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does</td>\n",
       "      <td>0.232568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>0.073792</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.202431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel</td>\n",
       "      <td>0.352740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>to</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>be</td>\n",
       "      <td>0.012990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>in</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>##t</td>\n",
       "      <td>0.020170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  gradient  type\n",
       "0      [CLS]  0.078237     0\n",
       "1       Does  0.232568     0\n",
       "2        the  0.073792     0\n",
       "3    patient  0.202431     0\n",
       "4       feel  0.352740     0\n",
       "..       ...       ...   ...\n",
       "217       to  0.010792     1\n",
       "218       be  0.012990     1\n",
       "219       in  0.024540     1\n",
       "220      ##t  0.020170     1\n",
       "221    [SEP]  0.031749     1\n",
       "\n",
       "[222 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_df = grad.get_gradients(model, tokenizer, question, context)\n",
    "grad_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = grad.html(grad_df['word'].tolist(), grad_df['gradient'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: hsla(110, 70%, 50%, 0.10476603880524635);\">[CLS]</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.21279790997505188);\">Does</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10165451169013977);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.19170177876949307);\">patient</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.29691818058490754);\">feel</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.75);\">numb</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.24502207040786744);\">?</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.14900603145360947);\">[SEP]</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09426288306713104);\">a</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.11192217096686363);\">##ys</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07882915772497653);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.13514921814203262);\">pain</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07857559509575367);\">mainly</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07080386504530907);\">goes</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06579878944903612);\">down</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06527294218540192);\">just</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06887021530419588);\">to</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05998219400644303);\">his</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07280113101005554);\">calf</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07957170382142068);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0662300018593669);\">not</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06814642120152711);\">really</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06225773710757494);\">his</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07397811263799667);\">foot</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06749756075441837);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.1175014354288578);\">says</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08494360111653805);\">occasionally</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07246352732181549);\">goes</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0602590473368764);\">to</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05877787712961435);\">his</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0683243265375495);\">foot</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07352332137525082);\">but</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.059629923291504384);\">is</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.061450745537877084);\">mostly</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.057322258502244955);\">in</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05914653986692429);\">his</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0716799920424819);\">thigh</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06474299039691686);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0680754316970706);\">calf</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07564385868608951);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06400691010057927);\">a</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07572693228721619);\">##gg</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06607467141002417);\">##ra</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08498976528644561);\">##vat</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06081345938146115);\">##ing</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06654880661517382);\">factor</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06973967179656029);\">activity</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05762873273342848);\">all</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.062058326043188575);\">##ev</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06561890617012978);\">##iating</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06403436660766601);\">factors</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06302901487797499);\">are</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06635279469192028);\">rest</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.082303424179554);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07407350242137908);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.11485097631812095);\">reports</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.15688266158103942);\">weakness</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07048124391585589);\">in</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06438969876617193);\">his</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06440086364746093);\">right</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06162382010370493);\">lower</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05924855452030897);\">ex</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06167489197105169);\">##tre</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0733682882040739);\">##mity</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06138805244117976);\">as</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06888732444494963);\">well</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.13275837674736976);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09822061508893967);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.13307222947478292);\">says</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0733694564551115);\">this</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09159706719219685);\">occurs</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07904939651489258);\">when</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07396881617605686);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06613246984779834);\">has</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10623558908700942);\">flare</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10112422481179237);\">##ups</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08039468452334404);\">but</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09344469271600246);\">currently</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07558088526129722);\">does</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09110172353684902);\">not</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07416083440184593);\">have</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0725257158279419);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.1287602201104164);\">weakness</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.19828174412250515);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.19596683382987973);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.6550209820270538);\">denies</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.17724462002515795);\">any</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.7214049279689789);\">numb</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.19947620779275893);\">##ness</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.18582974225282667);\">or</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.37554433345794674);\">tingling</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.5668837845325471);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.41142224073410033);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.30703477561473846);\">says</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.13189179003238677);\">had</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.11433151662349701);\">pain</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.13106682226061822);\">since</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.1703170582652092);\">1975</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07847516201436519);\">in</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07339287102222443);\">his</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09090698026120662);\">back</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.11842770725488662);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09078305698931217);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09478064253926277);\">occasionally</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08831054754555226);\">has</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10228519514203072);\">flare</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10730511248111724);\">##ups</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07087718714028597);\">but</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0709890367463231);\">these</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06325920075178146);\">have</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08655823096632957);\">gotten</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09456469342112542);\">worse</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06632343716919423);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09761430844664573);\">worse</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07622298002243041);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08818816542625427);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06281293798238038);\">is</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07205849625170231);\">done</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06631187852472067);\">physical</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07059195004403591);\">therapy</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0633617877960205);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0604251834563911);\">this</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06798551939427852);\">made</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06685796547681093);\">him</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0799198191612959);\">worse</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07238986231386661);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08316709548234939);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.058769501838833095);\">has</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0603916073217988);\">been</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06744493078440428);\">offered</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07248494699597359);\">injection</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.059134582895785576);\">##s</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06274341139942408);\">but</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06087713493034244);\">is</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.1036056213080883);\">refused</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06454507615417242);\">these</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07761438265442848);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07318193130195141);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.060726971644908194);\">is</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07878639139235019);\">concerned</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06390451472252608);\">about</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.059064683411270386);\">his</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08128763511776924);\">diabetes</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07685807831585408);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.11761519908905028);\">i</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0786572840064764);\">have</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07496945708990096);\">offered</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07448088824748993);\">him</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0767548367381096);\">injection</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06074149981141091);\">several</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.060498781315982346);\">times</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06495290789753198);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07075690813362598);\">he</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06395815256983042);\">does</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05927624376490712);\">not</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08106174543499947);\">wish</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06068138312548399);\">to</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06702007986605167);\">have</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06723423358052968);\">any</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08536178544163704);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09465025216341019);\">physical</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07987249195575714);\">examination</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.12309755086898803);\">:</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07839819565415382);\">alert</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06489032302051782);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0797360647469759);\">oriented</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.070209383033216);\">x</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0669971477240324);\">4</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06710818111896515);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.10663903206586837);\">patient</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06442433036863804);\">in</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06411382984369993);\">no</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0751497745513916);\">acute</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09470459669828415);\">distress</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07071559559553861);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06856521181762218);\">normal</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07050335723906756);\">affect</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06138560250401497);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0713515805080533);\">men</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08050192110240459);\">##tation</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08316426873207092);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08506927154958248);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.12500138804316518);\">patient</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.09941751435399056);\">stands</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06925291661173105);\">with</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.057625926192849876);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08877931870520114);\">shoulders</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06231256667524576);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05568038793280721);\">p</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05945554552599788);\">##el</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06243182476609946);\">##vis</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07007982190698385);\">level</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07717979587614536);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06699283067137003);\">sa</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06585429962724447);\">##git</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05816723387688399);\">##tal</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.062236054055392745);\">and</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08192910403013229);\">frontal</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05628993595018983);\">con</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07921158000826836);\">##tour</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05902819251641631);\">##s</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05911359479650855);\">of</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.057715215254575014);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07272242531180381);\">spine</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06507921461015939);\">are</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06373536996543408);\">well</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07937926836311818);\">maintained</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06841074321419001);\">.</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07919270806014538);\">examination</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06636588145047426);\">of</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.056123559921979906);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06329023372381926);\">low</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07316488735377788);\">back</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06917787473648787);\">region</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08306597135961055);\">reveals</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.057310891523957255);\">the</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.08287322819232941);\">skin</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05755427042022348);\">to</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.05909278076142073);\">be</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.06717805583029986);\">in</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.0641189057379961);\">##t</span><span> </span><span style=\"background-color: hsla(110, 70%, 50%, 0.07222427055239677);\">[SEP]</span><span> </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
